(15 puntos)  3.1   (5 puntos) Usando la definici´ on matricial de una regresi´ on lineal, derive el vector de coeficientes estimados usando el m´ etodo de m´ ınimos cuadrados ordinarios (MCO) ( hint:   e   =   y   −   Xβ ). Buscamos minimizar el cuadrado de la norma 2 del vector de errores, es decir, buscamos minimizar la siguiente funci´ on:  M CO ( β ) =   ϵ ′ ϵ   = ( y   −   Xβ ) ′ ( y   −   Xβ ) =   || y   −   Xβ || 2 2   (4) Podemos utilizar los conocimientos en ´ algebra matricial:  M CO ( β ) = ( y   −   Xβ ) ′ ( y   −   Xβ ) =   y ′ y   −   β ′ X ′ y   −   y ′ Xβ   +   β ′ X ′ Xβ  =   y ′ y   −   2 y ′ Xβ   +   β ′ X ′ Xβ  Sabemos que podemos minimizar una funci´ on utilizando el gradiente e igualando a cero  ∂M CO ( β )  ∂ β   =   ∇ M CO ( β ) =   − 2 X ′ y   + 2 X ′ Xβ   (5) Podemos utilizar (5) para obtener un candidato a m´ ınimo seg´ un  − 2 X ′ y   + 2 X ′ Xβ   = 0  X ′ Xβ   =   X ′ y   (6) De esta manera, las ecuaciones (6) se conocen como ecuaciones normales.   Verificamos que tenemos un m´ ınimo seg´ un el hessiano:  ∂ 2 M CO ( β )  ∂ β ∂ β ′   = 2 X ′ X   (7) Bajo el supuesto que las columnas de la matriz de dise˜ no son independientes, es decir, que la matriz es de rango columna completo, tenemos que la matriz hessiana presentada en (7) es definida positiva, luego tratamos con un m´ ınimo. Adem´ as, esto garantiza la existencia de la inversa de   X ′ X , luego podemos resolver las ecuaciones normales en (6) para   β : ˆ β   =   ( X ′ X ) − 1 X ′ y  Luego se tiene el resultado que se buscaba.  (1 punto)   Por determinar que la ecuaci´ on a minimizar es (4)  (3 puntos)   Por obtener correctamente el gradiente de la funci´ on en (4), i.e., por obtener la ecuaci´ on (6)  (1 punto)   Por concluir matem´ aticamente que se trata de un m´ ınimo, i.e. ecuaci´ on en (7)  3.2   (5 puntos) Demuestre por qu´ e es necesario incluir un vector de 1’s en   X   para que la suma de los errores sea cero. Denotemos   X 1   =   1 n   = [1 1   ...   1] ′   como el vector de 1’s de dimensi´ on   n . Luego,  X ′  1 e   =   1 ′  n e   =  n X  i =1  e i   (8) 5

--- Page 6 ---
Como los residuos son ortogonales a la matriz de dise˜ no, tenemos que   X ′  1 e   = 0, luego   P n i =1   e i   = 0.  (2.5 puntos)   Por plantear la ecuaci´ on (8)  (2.5 puntos)   Por utilizar la propiedad de que la matriz de dise˜ no es ortogonal a los residuos para concluir correctamente lo pedido  3.3   (5 puntos) La regresi´ on lineal puede entenderse como descomponer   y   en una proyecci´ on de   y   sobre  x   y sobre un vector de residuos   y .   Derive qu´ e matrices   M 1   (proyecci´ on) y   M 2   (aniquiladora) permiten descomponer   y   de la manera descrita. Utilizando el estimador MCO tenemos  ˆ β   = ( X ′ X ) − 1 X ′ y , luego considerando la regresi´ on   y   =   Xβ   +   ϵ , tenemos que  ϵ   =   y   −   Xβ  =   y   −   X ( X ′ X ) − 1 X ′ y  =   y ( I n   −   X ( X ′ X ) − 1 X ) Por lo tanto, definiendo   M 1   =   X ( X ′ X ) − 1 X   y   M 2   =   I n   −   M 1 , tenemos  y   =   M 1 y   +   M 2 y  Luego que   M 1   es una matriz de proyecci´ on y   M 2   es la matriz aniquiladora.  (2 puntos)   Por derivar   M 1   y   M 2  (3 puntos)   Por concluir correctamente que   M 1   es una matriz de proyecci´ on y   M 2   es la matriz aniquiladora. 6

--- Page 7 ---