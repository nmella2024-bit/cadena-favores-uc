---
title: "Documento Completo"
topic: "General"
number: "1"
originalUrl: "exports/downloads/Todos los ramos/Resumen ICS1113_03IxuxktxBGB30uecftE.pdf"
sourceFile: "Resumen ICS1113_03IxuxktxBGB30uecftE.pdf"
---

--- Page 1 ---
Resumen Optimizaci√≥n  ICS1113 - Prof. Jos√© Tom√°s Marquinez V. - 2016  Cap√≠tulo 0.- Investigaci√≥n de Operaciones.  Denici√≥n:   La   Investigaci√≥n de Operaciones   es una disciplina cient√≠ca que aplica m√©todos anal√≠- ticos avanzados para ayudar a tomar mejor decisiones. La metodolog√≠a incluye la   Denici√≥n del problema , la   Contrucci√≥n del problema , la   Soluci√≥n del modelo , la   Vericaci√≥n del modelo , y la   Implementaci√≥n y Control del modelo .  Cap√≠tulo 1.- Introducci√≥n al Modelamiento.  Denici√≥n:   Un   modelo   es un esquema te√≥rico, generalmente en forma matem√°tica, de un sistema o de una realidad compleja. Es una herramienta que ayuda a la toma de decisiones. Debe ser una simplicaci√≥n de la realidad.  Denici√≥n:   En particular, los   modelos matem√°ticos   pretenden optimizar. Existen: Din√°micos.  Est√°ticos .  Continuos .  Discretos .  Determin√≠sticos . Estoc√°sticos.  Lineales .  No Lineales .  Terminolog√≠a:   Un modelo matem√°tico cuenta con:  Variables de Decisi√≥n : Cantidades que se buscan determinar y que inciden en el objetivo.  Variables de Estado : Cantidades que buscan denir el estado del sistema o modelo.  Variables Auxiliares : Cantidades que buscan relacionar variables entre s√≠, principalmente.  Restricciones del problema : Relaciones entre las variables que limitan sus valores, de la forma  h j   ( ~ x ) = 0 , g i ( ~ x )   ‚â§   0 .  Restricciones de las variables : Limitaciones en los valores que pueden tomar las variables por s√≠ solas, de la forma   ~ x   ‚àà   Œ© : Œ©   ‚äÇ   R n . Ac√° caen las restricciones de   Naturaleza de las Variables .  Funci√≥n Objetivo : Medida para comparar las alternativas posibles. Se busca maximizar o minimizar este objetivo.  1

--- Page 2 ---
Resumen Optimizaci√≥n - ICS1113  Cap√≠tulo 2.- Conceptos b√°sicos.  Se   dene   el   espacio de soluciones factibles   Œ© , o simplemente espacio factible, como la intersecci√≥n de todas las restricciones. Si esta no incluye ning√∫n punto (si el espacio es vac√≠o), se le dice   espacio infactible . Se   dene   la   soluci√≥n o punto √≥ptimo   como el punto   ~ x   ‚àà   Œ©   que entrega el mejor valor de la funci√≥n objetivo. Se   dene   un   punto vecino   como aqu√©l punto   ~ x 2   que comparte todas las restricciones, excepto una, con un punto   ~ x 1 . Sea el siguiente problema de optimizaci√≥n:  P   )   m¬¥ ƒ±n   f   ( ~ x )  s.a.   ~ x   ‚àà   Œ©  Se   dene   un   punto extremo   como un punto que es m√≠nimo o m√°ximo, local o global, de una funci√≥n  f   sobre un dominio   Œ© . Un punto   ~ x   se   dene   m√≠nimo global   de   f   en   Œ©   (y soluci√≥n √≥ptima del problema   P   ) ), si  f   ( ~ x )   ‚â§   f   ( ~ y ) ,   ‚àÄ ~ y   ‚àà   Œ© .  Un punto   ~ x   se   dene   m√≠nimo local   de   f   en   Œ©   si existe un    >   0   tal que  f   ( ~ x )   ‚â§   f   ( ~ y ) ,   ‚àÄ ~ y   :   || ~ x   ‚àí   ~ y || ‚â§   , ~ y   ‚àà   Œ©  Se   denen   los   m√≠nimos estrictos   si las desigualdades son estrictas. Existencia de Soluciones √ìptimas:  Teorema:   El   Teorema de de Bolzano-Weierstrass   indica que si   f   es continua sobre un dominio   Œ©  no vac√≠o y compacto (cerrado y acotado), entonces el problema necesariamente tendr√° (al menos una)soluci√≥n √≥ptima (pues   f   alcanza sus puntos extremos).  Teorema:   El   Teorema Pr√°ctico de Existencia de Soluciones √ìptimas de la Optimizaci√≥n Lineal  dice que, en un problema lineal 1 , si el dominio   Œ©   es no vac√≠o y cerrado, y la funci√≥n objetivo est√° acotada inferiormente (en caso de minimizaci√≥n), entonces   P   )   tiene soluci√≥n √≥ptima.  Teorema:   El   Teorema de Existencia de Soluciones √ìptimas de la Hip√≥tesis H   dice que si   f  es continua sobre   Œ© , con   Œ©   cerrado y no vac√≠o sobre   R n , entonces si   H   :=   f   ( ~ x )   ‚Üí ‚àû   cuando  || ~ x || ‚Üí ‚àû , ~ x   ‚àà   Œ© , entonces   P   )   admite al menos una soluci√≥n √≥ptima. Problemas Equivalentes: Sean los siguientes problemas de optimizaci√≥n   P 1   y   P 2 :  P 1 )   m¬¥ ƒ±n   f   ( ~ x )   P 2 )   m¬¥ ƒ±n   h ( ~ y )  s.a.   g i ( ~ x )   ‚â§   0   i   = 1 , . . . , m   s.a.   l j   ( ~ y )   ‚â§   0   j   = 1 , . . . , r ~ x   ‚àà   Œ© 1   ~ y   ‚àà   Œ© 2  1 Ver Cap√≠tulo 3  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  2

--- Page 3 ---
Resumen Optimizaci√≥n - ICS1113  Se   dicen   problemas equivalentes   si la soluci√≥n √≥ptima de   P 1   provee la soluci√≥n √≥ptima de   P 2 , y viceversa.  Equivalencia I:  P 1 )   m¬¥ ƒ±n   f   ( ~ x )   ‚àº   P 2 )   m¬¥ ax   ‚àí f   ( ~ x )  s.a.   ~ x   ‚àà   Œ©   s.a.   ~ x   ‚àà   Œ©  Equivalencia II:  P 1 )   m¬¥ ƒ±n   f   ( ~ x )   ‚àº   P 2 )   m¬¥ ƒ±n   Œº  s.a.   ~ x   ‚àà   Œ©   s.a.   f   ( ~ x )   ‚â§   Œº ~ x   ‚àà   Œ© , Œº   ‚àà   R  Equivalencia III:  P 1 )   m¬¥ ƒ±n   m¬¥ ax { f 1 ( ~ x ) , . . . , f n ( ~ x ) }   ‚àº   P 2 )   m¬¥ ƒ±n   Œº  s.a.   ~ x   ‚àà   Œ©   s.a.   f i ( ~ x )   ‚â§   Œº   ‚àÄ i   = 1 , . . . , n ~ x   ‚àà   Œ© , Œº   ‚àà   R  Equivalencia IV:  P 1 )   m¬¥ ƒ±n  r ‚àë  i =1  f i ( ~ x )   ‚àº   P 2 )   m¬¥ ƒ±n  r ‚àë  i =1  Œº i  s.a.   ~ x   ‚àà   Œ©   s.a.   f i ( ~ x )   ‚â§   Œº i   ‚àÄ i   = 1 , . . . , r ~ x   ‚àà   Œ© , Œº i   ‚àà   R   ‚àÄ i   = 1 , . . . , r  Equivalencia V:  P 1 )   m¬¥ ƒ±n   f   ( ~ x )   ‚àº   P 2 )   m¬¥ ƒ±n   g ( f   ( ~ x ) )  s.a.   ~ x   ‚àà   Œ©   s.a.   ~ x   ‚àà   Œ©  donde la funci√≥n   g   :   f   (Œ©)   ‚äÇ   R   ‚Üí   R   es estrictamente creciente sobre   f   (Œ©) . Notemos que los valores √≥ptimos no ser√°n necesariamente los mismos.  Equivalencia VI:  P 1 )   m¬¥ ƒ±n   1  f   ( ~ x )   ‚àº   P 2 )   m¬¥ ax   f   ( ~ x )  s.a.   ~ x   ‚àà   Œ©   s.a.   ~ x   ‚àà   Œ©  con   f   ( ~ x )   >   0 ,   ‚àÄ ~ x   ‚àà   Œ© . Nociones B√°sicas de Convexidad: Se   dene   que un conjunto   Œ©   es un   conjunto convexo   si  ‚àÄ ~ x 1 , ~ x 2   ‚àà   Œ©   se tiene   ~ x   =   Œª~ x 1   + (1   ‚àí   Œª ) ~ x 2   ‚àà   Œ© ,   ‚àÄ Œª   ‚àà   [0 ,   1]  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  3

--- Page 4 ---
Resumen Optimizaci√≥n - ICS1113  Notemos que la intersecci√≥n de conjuntos convexos es convexa, la uni√≥n de conjuntos convexos no (necesariamente) es convexa, la desigualdad lineal es convexa. Luego, todos los poliedros denidos por desigualdades lineales son convexas. Se   dene   que   f   ( ~ x ) : Œ©   ‚Üí   R ,   Œ©   convexo, es una   funci√≥n convexa sobre   Œ©   si  f   ( Œª~ x 1   + (1   ‚àí   Œª )   ~ x 2 )   ‚â§   Œªf   ( ~ x 1 ) + (1   ‚àí   Œª ) f   (   ~ x 2 ) ,   ‚àÄ ~ x 1 ,  ~ x 2   ‚àà   Œ© , Œª   ‚àà   [0 ,   1] .  Una funci√≥n denida sobre un dominio no convexo no puede ser convexa. La suma de funciones convexas es convexa. La funci√≥n   f   es una   funci√≥n estrictamente convexa sobre   Œ©   si la desigualdad anterior es estricta. Se   dene   que   f   ( ~ x ) : Œ©   ‚Üí   R ,   Œ©   convexo, es una   funci√≥n c√≥ncava sobre   Œ©   si  f   ( Œª  ~ x 1   + (1   ‚àí   Œª )   ~ x 2 )   ‚â•   Œªf   (   ~ x 1 ) + (1   ‚àí   Œª ) f   (   ~ x 2 ) ,   ‚àÄ   ~ x 1 ,  ~ x 2   ‚àà   Œ© , Œª   ‚àà   [0 ,   1] .  Luego, si   f   ( ~ x )   es una funci√≥n convexa, entonces   ‚àí f   ( ~ x )   es una funci√≥n c√≥ncava. La funci√≥n   f   es una   funci√≥n estrictamente c√≥ncava sobre   Œ©   si la desigualdad anterior es estricta. Se   dene   que el problema de optimizaci√≥n   P   )   es un   problema convexo   si   Œ©   es una parte convexa de   R n   y   f   ( ~ x )   es convexa sobre   Œ© .  Teorema:   El   Teorema de Convexidad   indica que si, para el problema   P   )   convexo,   ~ x   es m√≠nimo local de   f   en   D , entonces   ~ x   es m√≠nimo global de   f   en   D . Sin embargo, no necesariamente este √≥ptimo ser√° √∫nico.  Corolario:   Si   f   ( ~ x )   es estrictamente convexo sobre   Œ©   (cerrado), entonces todo punto m√≠nimo local de   f   ( ~ x )   es tambi√©n su √∫nico m√≠nimo global.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  4

--- Page 5 ---
Resumen Optimizaci√≥n - ICS1113  Cap√≠tulo 3.- Introducci√≥n a la Programaci√≥n Lineal.  Denici√≥n:   Un   modelo de Programaci√≥n Lineal   es aquel cuyas variables son continuas, y tanto sus restricciones como su funci√≥n objetivo son lineales. Se da que en los Problemas de Programaci√≥n Lineal (PPL) continuos existen tres posibilidades: No existe soluci√≥n factible. El problema es no acotado. Existe (al menos) un punto extremo tal que es soluci√≥n √≥ptima (global) del problema. Soluci√≥n gr√°ca: Para determinar la soluci√≥n gr√°camente, es necesario gracar todas las restriccio- nes, determinar el espacio com√∫n de la intersecci√≥n de todas las restricciones, y gracar las curvas de nivel de la funci√≥n objetivo. En un problema de minimizaci√≥n, el (o los) punto que est√© asociado a la curva de nivel de menor valor, ser√° la soluci√≥n √≥ptima.  Denici√≥n:   Un   poliedro   es un conjunto denido por restricciones lineales (anes).  P   =   { x   ‚àà   R n   :   Ax   ‚â§   b }   o bien   P   =   { x   ‚àà   R n   :   Ax   =   b, x   ‚â•   0 }  Denici√≥n:   Si el poliedro es acotado, se le dice   pol√≠topo .  Teorema:   Sea   P   =   { x   ‚àà   R n   :   Ax   ‚â§   b }   no vac√≠o. El   Teorema de No Acotamiento   indica que   P   es un conjunto no acotado si y s√≥lo si   ‚àÉ h   ‚àà   R n , h   6   = 0   tal que   Ah   ‚â§   0 .  Denici√≥n:   Un vector   ~ x   se dice   punto extremo   (o v√©rtice) de   Œ©   si no existen   ~ u, ~ v   ‚àà   Œ© , ~ u   6   =   ~ v   tales que   ~ x   =   Œª~ u   + (1   ‚àí   Œª ) ~ v,   0   < Œª <   1 .  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  5

--- Page 6 ---
Resumen Optimizaci√≥n - ICS1113  Cap√≠tulo 4.- M√©todo Simplex en Programaci√≥n Lineal.  En este cap√≠tulo y el siguiente se omitir√° la notaci√≥n vectorial con echa a prop√≥sito.  Notaci√≥n:   Para iniciar, se considerar√° un PPL inicial escrito de la siguiente manera matricial:  m¬¥ ƒ±n   c T   x  s.a.   Ax   ‚â§   b  con   A   ‚àà   R n √ó m ;   c, x   ‚àà   R n ;   b   ‚àà   R m .  Denici√≥n:   Se dice que un PPL est√° escrito en su   formato est√°ndar   si se escribe de la siguiente manera matricial:   m¬¥ ƒ±n   c T   x  s.a.   Ax   =   b x   ‚â•   0  con   A   ‚àà   R n √ó m ;   b   ‚àà   R m ;   c, x   ‚àà   R n , tales que   n   ‚â•   m ;   b   ‚â•   0   y   A   tiene rango completo.  Denici√≥n:   La   fase II del m√©todo Simplex   es un algoritmo que itera viajando de un   punto factible  hacia su mejor vecino, hasta obtener el resultado deseado. Se basa en que si existe exactamente una soluci√≥n √≥ptima, entonces esta debe ser un v√©rtice, y si existen m√∫ltiples, al menos dos de ellas deben ser v√©rtices adyacentes (en problemas acotados). Y que si una soluci√≥n en un v√©rtice es igual o mejor que todas las soluciones factibles en los v√©rtices adyacentes a ella, entonces es igual o mejor que todas las dem√°s soluciones en los v√©rtices. Luego, es √≥ptima. El m√©todo requiere llevar el problema en su formato est√°ndar. Para ello: Si existen variables libres, se debe sustituir por la diferencia de dos variables auxiliares no negativas. Es decir,   x i   = Àú x i   ‚àí   Àú Àú x i . Si hay restricciones de desigualdades, se debe agregar   variables de holgura   para los casos de  ‚â§   o agregar   variables de exceso   (o restar variables de holgura) para los casos de   ‚â• . Si el problema requiere maximizar una funci√≥n, se debe cambiar al problema equivalente utilizando la Equivalencia I. Por notaci√≥n, el n√∫mero de restricciones es   m   y el n√∫mero de variables totales en este formato es   n .  Denici√≥n:   Una   base   es una submatriz de   A   formada por   m   columnas linealmente independientes. Permite despejar unas variables en t√©rminos de otras para jar algunas con el n de resolver un sistema de   n   √ó   n .  Denici√≥n:   Una   soluci√≥n b√°sica   del sistema   Ax   =   b, x   ‚â•   0   es una soluci√≥n de la forma   x   = [ x B   , x R ] , donde   x R   = 0 . Una vez escogida la base, se debe reordenar el problema para que quede escrito de la siguiente manera:  m¬¥ ƒ±n   z   =   c T B   ¬∑   x B   +   c T R   ¬∑   x R  s.a.   Bx B   +   Rx R   =   b x R , x B   ‚â•   0  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  6

--- Page 7 ---
Resumen Optimizaci√≥n - ICS1113  donde  x B   ‚àà   R m   tiene las coordenadas de   x   que est√°n consideradas en la base actual. Es decir, son las   variables b√°sicas . Por construcci√≥n, en la iteraci√≥n actual   x B   ‚â•   0 .  x R   ‚àà   R n ‚àí m   tiene las coordenadas de   x   que no est√°n consideradas en la base actual. Es decir, son las   variables no b√°sicas . Por construcci√≥n, en la iteraci√≥n actual   x R   = 0 .  B   es la   base   o matriz b√°sica.  R   es una submatriz de   A   formada por   n   ‚àí   m   columnas.  Denici√≥n:   Una soluci√≥n b√°sica del sistema   Ax   =   b, x   ‚â•   0   se llama   soluci√≥n b√°sica factible   si  x B   ‚â•   0 . Corresponde a un v√©rtice del dominio.  Denici√≥n:   Se dice que un PPL est√° escrito en su   formato can√≥nico   si se escribe de la siguiente manera matricial, asociada a una base particular (no necesariamente √≥ptima)   x p :  m¬¥ ƒ±n   z   = ( c p B   ) T   ¬∑   ¬Ø b p   + (¬Ø c p R ) T   ¬∑   x p R  s.a.   Ix p B   +  ¬Ø Rx p R   =  ¬Ø b x p B   ‚â•   0  x p R   = 0  Una iteraci√≥n de Simplex: I) Se calculan los siguientes componentes:  ¬Ø R   =   B ‚àí 1 R   ¬Ø b   =   B ‚àí 1 b  II)   ¬Ø b   indica el valor actual de las variables b√°sicas, las cuales deben ser positivas por construcci√≥n. Por lo tanto, se verica que se cumpla el   criterio de factibilidad   . En caso de no cumplirse, la base actual no es una base factible:  ¬Ø b   =   x B   =   B ‚àí 1 b   ‚â•   0  Si el valor de una variable b√°sica es igual a cero, existe soluci√≥n degenerada (punto sobrede- terminado). III) Se verica el   criterio de optimalidad   , que indica si la base actual es la base √≥ptima. En caso de que no, se debe continuar con la iteraci√≥n. El criterio indica que los   c ostos reducidos de las variables no b√°sicas no deben ser negativos (√©stos indican un aporte marginal a la funci√≥n objetivo). Si alguno de los costos reducidos es 0, entonces el problema tiene soluciones m√∫ltiples:  ¬Ø c R T   =   c T R   ‚àí   c T B   B ‚àí 1 R   =   c T R   ‚àí   c T B   ¬Ø R   = ‚â•   0  Si la base actual es la √≥ptima, el valor de las variables es   x R   = 0 , x B   =  ¬Ø b .  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  7

--- Page 8 ---
Resumen Optimizaci√≥n - ICS1113  IV) Se verica el   criterio de entrada . Entrar√° a la base aquella variable no b√°sica que tenga costos reducidos menores (entre las que poseen costos reducidos negativos). Si el criterio entrega m√°s de un   e , se escoge arbitrariamente uno para continuar. Sea   I R   el conjunto de √≠ndices de las variables no b√°sicas, entonces:  m¬¥ ƒ±n  ¬Ø c j   < 0 { ¬Ø c j   :   j   ‚àà   I R }   = ¬Ø c e   ‚Üí   x e   entra a la base. V) Se verica el   criterio de salida . Saldr√° de la base aquella variable que deba hacerse cero para llegar al v√©rtice vecino m√°s cercano. Si el criterio de salida entrega m√°s de un   s , se escoge arbitrariamente. Si el criterio de salida no entrega ning√∫n   s , el problema es no acotado. Sea  I B   el conjunto de √≠ndices de las variables b√°sicas, entonces:  m¬¥ ƒ±n  ¬Ø a ie > 0  {   ¬Ø b i  ¬Ø a ie  :   i   ‚àà   I B  }  =  ¬Ø b s  ¬Ø a se  ‚Üí   x s   sale de la base. VI) Se actualizan los componentes   x B   , x R , B   y   R   y se comienza nuevamente. Una iteraci√≥n de Simplex usando la t√©cnica de   tableau : Cabe mencionar que esta t√©cnica dej√≥ de verse en el curso ICS1113, sin embargo, este resumen lo incluye dado que antes se revisaba. I) Se escribe la siguiente matriz (la primera la es usada como una gu√≠a para el calculista de qu√© va en qu√© columna):  x B   x R   z x B   x R   z B   R   b  II)   Pivotear   la tabla completa de tal manera que en el cuadrante de B quede la matriz identidad, y los n√∫meros sobre esta matriz sean ceros. De esa manera, la matriz   pivoteada   quedar√° de la siguiente manera:  x B   x R   z  0   ¬Ø c R   z iter   =   ‚àí c B   B ‚àí 1 b I   ¬Ø R   ¬Ø b  III) Se verica el   criterio de optimalidad   . Se verica que todos los valores en la casilla de   ¬Ø c R   sean mayor o igual a cero. IV) Se verica el   criterio de entrada . Entrar√° a la base aquella variable no b√°sica que tenga costos reducidos menores (entre las que poseen costos reducidos negativos). Por lo tanto, entra a la base la variable que se encuentra inmediatamente arriba del menor valor de los   ¬Ø c R   en la gu√≠a para el calculista.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  8

--- Page 9 ---
Resumen Optimizaci√≥n - ICS1113  V) Se verica el   criterio de salida . Para ello, se destaca la columna de la variable que entra, y se calcula el cuociente de los valores entre la columna de   z   y la columna destacada (siempre y cuando estos √∫ltimos sean   >   0 ). Se debe destacar la la que contenga el menor cuociente entre los calculados. Luego, saldr√° aquella variable que se encuentre inmediatamente arriba (en la gu√≠a del calculista) del   pivote   (el 1 de la matriz identidad) de la la seleccionada. VI) Se cambia toda la columna de la variable entrante por la de la variable saliente y se comienza de nuevo.  Cap√≠tulo 5.- M√©todo de Simplex en su Fase I.  Denici√≥n:   La   fase I del m√©todo Simplex   utiliza el algoritmo de Simplex para encontrar una soluci√≥n factible del problema original. Para ello, agrega variables auxiliares que hacen factible una soluci√≥n que no lo es, al agregar dimensiones al problema. I) Se agregan   variables auxiliares   en aquellas restricciones que no presentan pivotes para la identidad que genera una soluci√≥n factible. Generalmente se agrega un variable   t j   por cada restricci√≥n   j   del problema. II) Se escoge la matriz   B   del problema como aquella que forme la matriz   identidad   . En el caso de haber agregado una variable por restricci√≥n, la base estar√° conformada por las variables auxiliares. III) Se intenta que las variables auxiliares no sean necesarias para estar en una soluci√≥n factible del problema original, por lo que se modica la funci√≥n objetivo a la   suma simple de todas las variables auxiliares   que se hayan agregado. IV) Se comienza a iterar Simplex (como si fuese Fase II) de este nuevo problema auxiliar hasta que el criterio de optimalidad lo indique. V) Si la base actual es tal que el valor √≥ptimo de Fase I es igual a 0, entonces esa es la base que se utiliza para comenzar a iterar Fase II del problema original. Si la base actual es tal que el valor √≥ptimo de Fase I es mayor a 0, entonces no existe soluci√≥n factible para comenzar a iterar Simplex Fase II del problema original.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  9

--- Page 10 ---
Resumen Optimizaci√≥n - ICS1113  Cap√≠tulo 6.- An√°lisis de Sensibilidad.  Denici√≥n:   Se le dice   An√°lisis de Sensibilidad   al arte de modicar par√°metros del problema para ver c√≥mo se comporta la soluci√≥n del mismo. Asume una soluci√≥n √≥ptima   x ‚àó   obtenida de una base √≥ptima   B ‚àó . Variaci√≥n en los recursos   b j   :  Denici√≥n:   Los   precios sombra   indican el cambio global en la funci√≥n objetivo al variar en una unidad el lado derecho de una restricci√≥n (una componente del vector   b ).  œÄ T   =  ‚àÜ z  ‚àÜ b   =   c T B   ¬∑   [ B ‚àó ] ‚àí 1  Notemos que para el c√°lculo de   œÄ T   se requiere que no cambie   B ‚àó . Por lo tanto, se requiere estudiar el criterio de factibilidad. Variaci√≥n en los costos   c i : Esto var√≠a la pendiente de las curvas de nivel, por lo que podr√≠a cambiar la base √≥ptima. Requiere que el criterio de optimalidad de Simplex siga cumpli√©ndose. Variaci√≥n en los factores tecnol√≥gidos   a ij   : Generalmente requiere un estudio desde cero, porque ocurren ambas cosas simult√°neamente. Agregaci√≥n de una variable   x n +1 : Proceder como si hubiera cambiado un costo no b√°sico, ya que no cambia el tama√±o de la base. Agregaci√≥n de una restricci√≥n: Vericar si la soluci√≥n actual sigue siendo factible. Notar que en este caso la base s√≠ cambia su tama√±o.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  10

--- Page 11 ---
Resumen Optimizaci√≥n - ICS1113  Cap√≠tulo 7.- Teor√≠a de Dualidad.  Denici√≥n:   Sea el problema lineal siguiente:  P   )   z ‚àó   =   m¬¥ ƒ±n   z   =   ~ c T   ~ x  s.a.   A~ x   ‚â•   ~ b ~ x   ‚â•   0  Se le llama   problema dual a   P   al problema  D )   w ‚àó   =   m¬¥ ƒ±n   w   =   ~ y T ~ b  s.a.   A T   ~ y   ‚â•   ~ c ~ y   ‚â•   0  Para pasar de primal a dual, se requiere hacer lo siguiente: I) Por cada restricci√≥n   j   del problema primal se dene una variable dual   y j   . II) Los costos del problema dual ser√°n los recursos del problema primal   ~ b . III) Los recursos del problema dual ser√°n los costos del problema primal   ~ c . IV) La matriz A ser√° transpuesta, y las variables que la acompa√±an ser√°n las variables   ~ y . V) Los signos de desigualdad de las restricciones y de la naturaleza variable del problema dual va directamente relacionada con los del primal, seg√∫n la siguiente tabla:  Minimizaci√≥n   Maximizaci√≥n Variables   Restricciones  ‚â•   0   ‚â§ ‚â§   0   ‚â•  Irrestrictas   =  Restricciones   Variables  ‚â§   ‚â§   0  ‚â•   ‚â•   0 =   Irrestrictas  Teorema:   El   Teorema D√©bil de Dualidad   nos dice lo siguiente: Sea   ~ x   una soluci√≥n factible del problema primal y sea   ~ y   una soluci√≥n factible del problema dual. Entonces, si el problema primal es de minimizaci√≥n en   ~ x   y el dual de maximizaci√≥n en   ~ y , entonces   ‚àÄ ~ x, ~ y   factibles se cumple que  z ( ~ x ) =   ~ c T   ~ x   ‚â•   ~ b T   ~ y   =   w ( ~ y ) .  Corolario:   Si alguno de los problemas es factible y no acotado, entonces el otro problema es infactible.  Corolario:   Si alguno de los problemas es factible y el otro es infactible, entonces el problema que admite soluci√≥n factible es no acotado.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  11

--- Page 12 ---
Resumen Optimizaci√≥n - ICS1113  Teorema:   El   Lema de Farkas   nos dice lo siguiente: Considerando un problema en su forma est√°ndar con una funci√≥n objetivo cualquiera   ~ c , y su dual, entonces  Ax   =   b, x   ‚â•   0   infactible   ‚áî   ‚àÉ y   6   = 0 :   A T   y   ‚â§   0 , b T   y >   0  Teorema:   El   Teorema Fuerte de Dualidad   nos dice lo siguiente: Dado un par de problemas primal- dual, si uno de ellos admite soluci√≥n √≥ptima, entonces el otro tambi√©n la admite y los respectivos valores √≥ptimos son iguales:  z ( ~ x ‚àó ) =   ~ c T   ~ x ‚àó   =   ~ b T   ~ y ‚àó   =   w ( ~ y ‚àó )  Cada base √≥ptima en el primal   mapea   una base √≥ptima en el dual. Se tiene, tambi√©n, que   ~ y ‚àó   =   ~ œÄ ‚àó , es decir, los valores de las variables duales en su √≥ptimo representan los precios sombras de las restricciones respectivas en el primal. Las holguras del dual son los costos reducidos del primal (y en el √≥ptimo son   ‚â•   0 ). Adem√°s, si   ~ x   no es √≥ptimo para el problema   P   , entonces   ~ y   no es factible para el problema   D .  Teorema:   El   Teorema de Holgura Complementaria   nos dice que para el problema   P   y el problema  D , ambos factibles, se tiene que si   ~ x ‚àó   es √≥ptima del primal y si   ~ y ‚àó   es √≥ptima del dual, entonces  y ‚àó  i  (  b i   ‚àí  n ‚àë  j =1  a ij   x ‚àó  j  )  = 0 ,   i   = 1 , . . . , m x ‚àó  j  (  c j   ‚àí  m ‚àë  i =1  a ij   y ‚àó  i  )  = 0 ,   j   = 1 , . . . , n  Es decir, si una restricci√≥n no es activa, entonces su correspondiente variable dual es nula. Con respecto a las variables, se obtienen las siguientes relaciones (son bidireccionales):  En el Primal   En el Dual  Variables   Costos Reducidos asociados a Holguras Holguras   Costos Reducidos asociados a Variables Variables B√°sicas   Costos Reducidos de Variables No B√°sicas Variables No B√°sicas   Costos Reducidos de Variables B√°sicas. M√∫ltiples Soluciones   Soluci√≥n Degenerada  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  12

--- Page 13 ---
Resumen Optimizaci√≥n - ICS1113  Cap√≠tulo 8.- Programaci√≥n Lineal Entera.  Denici√≥n:   Un   modelo de Programaci√≥n Lineal Entera   (PLE) es aqu√©l cuyas variables son enteras, y tanto sus restricciones como su funci√≥n objetivo son lineales. Existen dos tipos que se ven: Problema   entero puro : todas las variables son enteras.  PE   )   z ‚àó   =   m¬¥ ƒ±n   z   =   ~ c T   ~ x  s.a.   A~ x   ‚â•   ~ b ~ x   ‚àà   Z +  Problema   entero binario : todas las variables toman valores 0 √≥ 1.  P B )   z ‚àó   =   m¬¥ ƒ±n   z   =   ~ c T   ~ x  s.a.   A~ x   ‚â•   ~ b ~ x   ‚àà   [0 ,   1]  Denici√≥n:   Un   modelo de Programaci√≥n Lineal Entero Mixto   (MILP) es aqu√©l con algunas variables enteras y/o binarias y con otras continuas, y tanto sus restricciones como su funci√≥n objetivo son lineales ( 1   ‚â§   s   ‚â§   p < n ).  MILP   )   z ‚àó   =   m¬¥ ƒ±n   z   =   ~ c T   ~ x  s.a.   A~ x   ‚â•   ~ b x j   ‚â•   0 ,   ‚àÄ j   = 1 , . . . , s x j   ‚àà   [0 ,   1] ,   ‚àÄ j   =   s   + 1 , . . . , p x j   ‚àà   Z + ,   ‚àÄ j   =   p   + 1 , . . . , n  Denici√≥n:   Sea el problema lineal entero mixto   PLM   . El siguiente problema se llama   relajaci√≥n lineal   del problema entero mixto original.  PR )   z 0   =   m¬¥ ƒ±n   z   =   ~ c T   ~ x  s.a.   A~ x   ‚â•   ~ b ~ x   ‚â•   0 0   ‚â§   x j   ‚â§   1 ,   ‚àÄ j   =   s   + 1 , . . . , p  Denici√≥n:   Una matriz   A   es   totalmente unimodular   si el determinante de cada una de sus sub- matrices cuadradas es 0, 1 √≥   ‚àí 1 .  Teorema:   Si la matriz   A   es totalmente unimodular y   ~ b   es un vector con todos sus valores enteros, entonces el poliedro   P   =   { ~ x   :   A~ x   =   ~ b }   tiene todos sus puntos extremos enteros.  Branch & Bound   : (o Ramicaci√≥n y Acotamiento): Algoritmo que ramica el espacio de soluciones cada vez que obtiene una soluci√≥n fraccionaria en alguna componente.  Denici√≥n:   En este contexto, se le llama   incumbente   a la mejor soluci√≥n entera obtenida hasta el instante. Supongamos que se desea resolver el problema   P   , cuyo espacio factible sin considerar la naturaleza variable denotaremos   R 0 :  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  13

--- Page 14 ---
Resumen Optimizaci√≥n - ICS1113  I) Se resuelve el problema relajado lineal del problema,   P R . Si la soluci√≥n es entera, no se acota m√°s la rama. Si el valor de la funci√≥n objetivo es peor que el valor del incumbente, no se acota m√°s la rama. Si el problema actual es infactible, no se resuelve ni se ramica. Si la soluci√≥n posee alguna componente fraccionaria, digamos la componente   k , entonces se debe ramicar. II) Si de decidi√≥ ramicar, entonces se crean dos subproblemas   P R 1   y   P R 2 , cuyos dominios ser√°n: Para   P R 1   ‚Üí   R 1   =   R 0   ‚à© { x   :   x k   ‚â• d x k e}  Para   P R 2   ‚Üí   R 2   =   R 0   ‚à© { x   :   x k   ‚â§ b x k c}  III) Se contin√∫a con cada rama reiniciando el algoritmo recursivamente. Se tiene que el valor √≥ptimo de las ramas ser√° siempre peor al valor √≥ptimo del nodo desde donde fue ramicado. A su vez, se puede   denir   el   GAP   o Brecha de Optimalidad, que indica qu√© tan lejos estamos del √≥ptimo, con   z RL   el mejor valor no entero actual y   z L   el mejor valor entero actual:  Œ≤   =   z RL   ‚àí   z L  z RL  Sin embargo, es m√°s pr√°ctico y claro ir deniendo las mejores cotas superiores y cotas inferiores para el problema entero que se tiene hasta el momento. Planos Cortantes:  Denici√≥n:   Un   plano cortante   es una restricci√≥n redundante para la formulaci√≥n del problema pero que corta soluciones fraccionarias de la relajaci√≥n lineal. De esta manera, se intenta hacer que el poliedro formado por las restricciones tengan como v√©rtices soluciones enteras, como se muestra en la siguiente gura. Esto se puede hacer con la ayuda del gr√°co del problema, y corresponde a la envoltura convexa.  Denici√≥n:   La   envoltura convexa   C ( X )   de un conjunto de puntos   Œ©   es la intersecci√≥n de todos los conjuntos convexos que contienen a   Œ© .  C ( X ) =  Ô£± Ô£≤ Ô£≥  | Œ© |  ‚àë  i =1  Œ± i x i  ‚à£ ‚à£ ‚à£ ‚à£   ( ‚àÄ Œ± i   :   Œ± i   ‚â•   0)   ‚àß  | Œ© |  ‚àë  i =1  Œ± i   = 1  Ô£º Ô£Ω Ô£æ  Ax   ‚â§   b   ‚áí  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  14

--- Page 15 ---
Resumen Optimizaci√≥n - ICS1113  T√©cnica de Covers: Consideremos restricciones del tipo  n ‚àë  i =1  a i x i   ‚â§   b,   con   a i   ‚â•   0   ‚àÄ i   = 1 , . . . , n   y   x i   ‚àà { 0 ,   1 }   ‚àÄ i   = 1 , . . . , n  Denici√≥n:   Un   cover   C   es un subconjunto de   { 1 , . . . , n }   tal que   ‚àë  i ‚àà C  a i   > b .  Teorema:   De un   cover   C , se puede denir el siguiente plano cortante v√°lido:  ‚àë  i ‚àà C  x i   ‚â§ | C | ‚àí   1  M√©todo de Gomory:  Denici√≥n:   El   M√©todo de Gomory   es una t√©cnica antigua de corte del espacio, utilizando un algoritmo claro de resoluci√≥n que impide que la soluci√≥n √≥ptima del problema relajado sea soluci√≥n del pr√≥ximo problema a resolver, sin exclu√≠r soluciones enteras del espacio. Este es el siguiente: I) Resolver el problema de relajaci√≥n lineal   P R . Si la soluci√≥n resultante tiene todas las compo- nentes enteras, no seguir. Si alguna es entera, realizar un corte. II) Obligar a que se cumpla lo siguiente:  ‚àë  j ‚àà I R  (¬Ø a kj   ‚àí b ¬Ø a kj   c )( x R ) j   ‚â•   ( ¬Ø b k   ‚àí b ¬Ø b k c )  en donde   I R   es el conjunto de √≠ndices de las variables no b√°sicas,   ¬Ø a kj   son coecientes de   B ‚àí 1 R  y   ¬Ø b   =   B ‚àí 1 ~ b . De esta manera, se obtienen cortes v√°lidos para los valores de las variables no b√°sicas. De las restricciones del problema est√°ndar se pueden obtener igualdades que permitan escribir estas nuevas restricciones en t√©rminos de lo que convenga (en particular, de las variables principales para poder gracarlo). La restricci√≥n denida en (II) se   dene   como   corte de Gomory   . Es √∫til mezclar los m√©todos, y ejecutar   Branch & Bound   con algunos cortes inclu√≠dos en el problema, para fortalecer la formulaci√≥n.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  15

--- Page 16 ---
Resumen Optimizaci√≥n - ICS1113  Cap√≠tulo 9.- Flujo en Redes.  Se asumen conocimientos b√°sicos de grafos, √°rbol, camino y circuito. Problema de Flujo a Costo M√≠nimo: Se busca determinar los ujos en los arcos de la red de modo que los elementos ofrecidos sean transportados hasta los puntos de demanda al menor costo posible, satisfaciendo las restricciones de capacidad y ujo m√≠nimo por arco.  Notaci√≥n:   Para estos problemas se tiene un grafo dirigido   G   = ( N, A ) , en el que a cada   nodo  i   ‚àà   N   ( | N   |   =   n ) se le asocia una   oferta   b i   (demanda si es   <   0 ), con  n ‚àë  i =1  b i   = 0 . A cada   arco  ( i, j )   ‚àà   A   se le asocia un   costo unitario   de transporte   c ij   , una   cota m√≠nima   de transporte   l ij   ‚â•   0  una   capacidad m√°xima   de transporte   u ij   ‚â•   l ij   . El modelo quedar√° dado, con   x ij   la cantidad de productos que ir√°n del nodo   i   al nodo   j   por el arco  ( i, j )   ‚àà   A , por  m¬¥ ƒ±n   ‚àë  ( i,j ) ‚àà A  c ij   x ij  s.a.   ‚àë  j :( i,j ) ‚àà A  x ij   +   ‚àë  k :( k,i ) ‚àà A  x ki   =   b i   ‚àÄ i   ‚àà   N   Conservaci√≥n de ujo  x ij   ‚â§   u ij   ‚àÄ ( i, j )   ‚àà   A   Capacidad m√°xima  x ij   ‚â•   l ij   ‚àÄ ( i, j )   ‚àà   A   Capacidad m√≠nima (no negatividad)  Para los alcances del curso ICS1113, considerar   l ij   = 0   y   u ij   =   ‚àû ,   ‚àÄ ( i, j )   ‚àà   A . La estructura de estos problemas tienen una bondad adicional debido a que la matriz de restricciones del problema es totalmente unimodular (tambi√©n se dice que es de capacidad m√°xima). Simplex Especializado en Redes: Permite resolver los problemas de ujo a costo m√≠nimo y an√°logos. Se mueve de un √°rbol generador (o base) a otro, hasta encontrar el √≥ptimo. I) Se debe encontrar un   criterio de factibilidad   de la red, es decir, un grafo conexo y sin circuitos que abarque todos los nodos del problema sin generar circuitos. Adem√°s, este grafo debe permitir un ujo inicial factible. II) Con la ayuda del √°rbol, determinar las variables b√°sicas y las no b√°sicas. Las   variables b√°sicas  son aquellos ujos   x ij   tales que   l ij   ‚â§   x ij   ‚â§   u ij   . Las   variables no b√°sicas   son aquellas que  x ij   =   l ij   √≥   x ij   =   u ij   . Luego, es f√°cil ver que todo ujo que cumple   l ij   < x ij   < u ij   debe ser un ujo b√°sico. III) Dado que los   costos reducidos de las variables b√°sicas   deben ser 0, se calculan los valores de las variables duales   œÄ i , teniendo en cuenta que los grados de libertad del problema permiten jar una de esas variables arbitrariamente.  ¬Ø c ij   =   c ij   ‚àí   œÄ i   +   œÄ j   = 0  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  16

--- Page 17 ---
Resumen Optimizaci√≥n - ICS1113  IV) Se verica el   criterio de optimalidad   , que indica si la base actual es √≥ptima. Para ello, se verican que se cumplan las siguientes condiciones para las variables   no   b√°sicas:  ¬Ø c ij   =   c ij   ‚àí   œÄ i   +   œÄ j   ‚â•   0   si   x ij   =   l ij  ¬Ø c ij   =   c ij   ‚àí   œÄ i   +   œÄ j   ‚â§   0   si   x ij   =   u ij  Si el criterio no se cumple, entonces se debe continuar al siguiente punto. Si el criterio se cumple, entonces se termina de iterar, y la base actual es la √≥ptima. Si el valor de alguno de estos costos reducidos es cero, el problema presenta m√∫ltiples soluciones. V) Se verica el   criterio de entrada , que indica qu√© variable entrar√° a la base, es decir, cu√°l arco se incorpora al √°rbol actual. Si el criterio entrega m√°s de un par   ( p, q ) , se escoge arbitrariamente uno para continuar. La variable que entra ser√° aquella con m√°xima violaci√≥n de optimalidad.  m¬¥ ƒ±n {   m¬¥ ƒ±n  ¬Ø c ij   < 0   si   x ij   = l ij  { ¬Ø c ij   }   ,   ‚àí   m¬¥ ax  ¬Ø c ij   > 0   si   x ij   = u ij  { ¬Ø c ij   } }   = ¬Ø c pq   ‚Üí   x pq   entra a la base. VI) Al ingresar el arco   ( p, q )   a la base, se generar√° un circuito en el √°rbol generador, que indicar√° los arcos cuyo ujo se ver√° modicado por la reasignaci√≥n. El   criterio de salida   nos indica cu√°l es el arco que acotar√° dicha reasignaci√≥n de ujo    . Ese arco, una vez reasignado el ujo, ser√° el que salga de la base. Si el criterio de salida entrega m√°s de un par   ( r, s ) , se escoge arbitrariamente uno para continuar:  m¬¥ ƒ±n { u pq   ‚àí   l pq   ,   m¬¥ ƒ±n  ( i,j ) ‚àà C 1  { x ij   ‚àí   l ij   }   ,   m¬¥ ƒ±n  ( i,j ) ‚àà C 2  { u ij   ‚àí   x ij   } }   =    rs   ‚Üí   x rs   sale de la base. donde   C 1   son los arcos del circuito formado que siguen el sentido del ujo, y   C 2   son los arcos del circuito formado que siguen el sentido contrario del ujo. La variable entrante domina el sentido del ujo. Si el criterio de salida no entrega ning√∫n par   ( r, s ) , entonces el problema es un problema no acotado. Notemos que puede ocurrir que la base no cambie (en el caso de que el m√≠nimo sea   u pq   ‚àí   l pq ), ya que para encontrar el punto de la iteraci√≥n siguiente s√≥lo bastaba reasignar el ujo. VII) Se   actualizan   los ujos, haciendo:  x nuevo  ij   =   x ij   +      si   x ij   ‚àà   C 1  x nuevo  ij   =   x ij   ‚àí      si   x ij   ‚àà   C 2  y se vuelve a III. An√°lisis de Sensibilidad para Flujo a costo m√≠nimo: Sigue el mismo criterio que Simplex lineal, analogiz√°ndolo de manera correcta. Problema de la Ruta m√°s Corta: Consiste en determinar un camino dirigido en el grafo que conecte a un nodo   s   con otro nodo   t   a costo m√≠nimo.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  17

--- Page 18 ---
Resumen Optimizaci√≥n - ICS1113  Denotaci√≥n:   Para estos problemas se tiene un grafo dirigido   G   = ( N, A ) , con   | N   |   =   n , en el que cada   arco   ( i, j )   ‚àà   A   tiene una se le asocia un   costo   de transporte   c ij   . Se denotan dos nodos como especiales: un   nodo de origen   s   ‚àà   N   y un   nodo de destino   t   ‚àà   N   . El modelo quedar√° dado, con   x ij   variable binaria relajada que indica si se usar√° el arco   ( i, j )   para llegar de   r   a   s , por  m¬¥ ƒ±n   ‚àë  ( i,j ) ‚àà A  c ij   x ij  s.a.   ‚àë  j :( i,j ) ‚àà N  x ij   ‚àí   ‚àë  k :( k,i ) ‚àà A  x ki   =  Ô£± Ô£≤ Ô£≥  1   si   i   =   s  ‚àí 1   si   i   =   t  0   si   i   6   =   s, t  ‚àÄ i   ‚àà   N x ij   ‚àà { 0 ,   1 }   ( i, j )   ‚àà   A  Algoritmo de   Dijkstra :   Iterativamente determina el camino m√°s corto para llegar a un nodo nuevo hasta determinar el del nodo origen. I) El conjunto   V   de nodos se particiona en dos subconjuntos   V   (tambi√©n llamado   S ) y   NV  (tambi√©n llamado   ( V   ‚àí   S ) ). Un nodo pertenece a   V   si se conoce la ruta m√°s corta para llegar a √©l desde el nodo origen   s . Por lo tanto,   NV   contiene el resto de los nodos. Para cada nodo   i   se dene un valor   d i   =   ‚àû   que indica la distancia o costo total necesario para llegar al nodo   i   utilizando el mejor camino, y un valor   p i   =   j   que indica el nodo   j   desde el cual es conveniente entrar al nodo   i . II) Sea   V   =   s ,   d s   = 0 ,   n s   =   s . III) Para el √∫ltimo nodo   i   en ingresar a   V   , se eval√∫an todos los nodos   j / ‚àà   V   que tienen en com√∫n un arco con   i . Para todos esos nodos   j , se actualiza el valor  d j   ‚Üê   m¬¥ ƒ±n { d j   , d i   +   c ij   } ,   ‚àÄ ( i, j )   ‚àà   A   :   j   ‚àà   NV  IV) Si   d antiguo  j   < d nuevo  j   , entonces actualizar   p j   ‚Üê   i . V) Sea   k   ‚àà   ( N V   )   tal que  d k   = m¬¥ ƒ±n { d i   :   i   ‚àà   N V   }  VI) Si   k   =   t , entonces seguir al paso VII). Si   k   6   =   t , hace   V   ‚Üê   V   ‚à™ { k } ,   N V   ‚Üê   N V   ‚àí { k }   y retomar en el paso III). VII) El camino √≥ptimo para llegar del nodo   s   al nodo   t   queda determinado por los valores   p v , reconstruyendo el camino desde el destino hacia el origen. Notemos que el algoritmo permite encontrar el camino √≥ptimo desde un nodo origen hacia cualquier nodo   i   si se decide que el algoritmo termine una vez que   V   =   N   .  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  18

--- Page 19 ---
Resumen Optimizaci√≥n - ICS1113  Problema de Flujo M√°ximo: Consiste en determinar la m√°xima cantidad de ujo que se puede transportar a trav√©s de una red desde un nodo de origen a otro de destino, dada las capacidades de los arcos.  Denotaci√≥n:   Para estos problemas se tiene un grafo dirigido   G   = ( N, A ) , en el que cada   arco  ( i, j )   ‚àà   A   tiene una se le asocia una   cota m√≠nima   de transporte   l ij   y una   capacidad m√°xima   de transporte   u lj   , y no tiene costo asociado. Existen dos nodos especiales: un   nodo de origen   s   ‚àà   N   y un   nodo de destino   t   ‚àà   N   . El modelo quedar√° dado, con   x ij   el ujo en el arco   ( i, j )   y   F   el valor del ujo que entra en el origen, por  m¬¥ ax   F  s.a.   ‚àë  j :( i,j ) ‚àà A  x ij   ‚àí   ‚àë  k :( k,i ) ‚àà A  x ki   =  Ô£± Ô£≤ Ô£≥  F   si   i   =   s  0   si   i   6   =   s, t  ‚àí F   si   i   =   t  Ô£º Ô£Ω Ô£æ   ‚àÄ i   ‚àà   N l ij   ‚â§   x ij   ‚â§   u ij   ( i, j )   ‚àà   A  Denici√≥n:   Un   corte de la red que separa   s   y   t , denotado como corte- ( s, t ) , es una partici√≥n de   N  en dos subconjuntos,   S   y   T   , tales que   s   ‚àà   S   y   t   ‚àà   T   .   Denici√≥n:   La   capacidad   o valor de corte est√° dada por cap ( S, T   ) =   ‚àë  ( i,j ) ‚àà A : i ‚àà S,j ‚àà T  u ij  El corte con la capacidad m√°s peque√±a se   denomina   corte m√≠nimo   y limita el ujo de la red. Es decir,   F   ‚â§   cap ( S, T   ) ,   ‚àÄ ( S, T   ) :   S   ‚äÜ   V   ‚àí { t } , s   ‚àà   S, t   ‚àà   T   .  Teorema:   El   Teorema de Ford & Fulkerson   indica que el m√°ximo valor de ujo (si existe) que se puede enviar desde el origen al destino es igual a la capacidad de un corte m√≠nimo. Es decir   Teorema:  Adem√°s, se tiene que si   F   ‚àó   es el ujo m√°ximo entre   s   y   t , entonces   F   ‚àó   = m¬¥ ƒ±n ( S,T   ) { cap ( S, T   ) } .  Denici√≥n:   Se llama   grafo de capacidades residuales   al grafo   G ‚Ä≤   = ( N, A ‚Ä≤ ) , en que   A   es: Para   a   = ( i, j )   ‚àà   A , si   f a   < u a , se pone un arco   ( i, j )   con capacidad igual a   u a   ‚àí   f a . Para   a   = ( i, j )   ‚àà   A , si   f a   > l a   = 0 , se pone un arco   ( j, i )   con capacidad igual a   f a .  Algoritmo de Ford-Fulkerson:   Construye iterativamente el ujo m√°ximo a partir de un ujo factible, en que en cada iteraci√≥n se identica un camino de aumento de ujo con respecto al ujo existente, a partir de la red residual. I) Se construye el grafo residual (o de capacidades residuales) a partir de alg√∫n ujo factible   ~ f  (o   ~ x ), que tiene asociado un ujo saliente de   s   (o entrante a   t ) igual a   F   . II) Mientras exista un camino   p   desde   s   hacia   t   en la red residual, se calcula la capacidad m√°xima de dicho camino:  f p   = m¬¥ ƒ±n  ( i,j ) ‚àà p { u ij   }  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  19

--- Page 20 ---
Resumen Optimizaci√≥n - ICS1113  III) A todos los arcos   ( i, j )   ‚àà   p   se le resta la cantidad   f p   en la red. Es decir, se asume que se pasar√° esa cantidad por camino. Se recomienda anotar en cada arco la cantidad   f p   de esa iteraci√≥n, para despu√©s saber cu√°l es el ujo por cada arco. Se vuelve al paso II). IV) Una vez que no queden caminos posibles de   s   a   t   con   f p   >   0 , se ha terminado el algoritmo. El ujo m√°ximo es  F   ‚àó   =   F   ‚àë  p   encontrados  f p  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  20

--- Page 21 ---
Resumen Optimizaci√≥n - ICS1113  Cap√≠tulo 10.- Optimizaci√≥n No Lineal Sin Restricciones.  Esta secci√≥n considerar√° un problema est√°ndar  m¬¥ ƒ±n   f   ( ~ x )  s.a.   ~ x   ‚àà   R n  Denici√≥n:   Un   modelo de Programaci√≥n No Lineal   es aqu√©l cuyas variables son continuas, y alguna de sus restricciones o funci√≥n objetivo son no lineales.  Teorema:   (Condici√≥n necesaria)   Sea   f   continuamente diferenciable. Si   x ‚àó   es m√≠nimo local de   f   , entonces   ‚àá f   ( x ‚àó ) =   ~ 0 .  Denici√≥n:   Un   punto estacionario   o punto cr√≠tico es aqu√©l punto   x ‚àó   que satisface el teorema anterior. Estos puntos se pueden identicar como candidatos a m√≠nimo.  Teorema:   (Condici√≥n necesaria de segundo orden)   Sea   f   :   R n   ‚Üí   R   una funci√≥n de clase   C 2 . Si  x ‚àó   ‚àà   R n   un m√≠nimo local de   f   ( ~ x ) , entonces se tiene que   ‚àá f   ( ~ x ) =   ~ 0   y que   H f   ( x ‚àó ) =   ‚àá 2 f   ( x ‚àó )   es semidenida positiva 2 .  Teorema:   (Condici√≥n suciente)   Sea   f   :   R n   ‚Üí   R   una funci√≥n de clase   C 2 . Sea el punto   x ‚àó   un punto cr√≠tico de   f   . Si   H f   ( x ‚àó ) =   ‚àá 2 f   ( x ‚àó )   es denida positiva,   x ‚àó   es m√≠nimo local estricto de   f   .  Teorema:   Una funci√≥n   f   es   convexa   en   Œ©   si y solo si   H f   ( ~ x )   es una matriz semidenida positiva, para todo   ~ x   ‚àà   Œ© . (Y es c√≥ncava si y solo si es una matriz semidenida negativa).  Teorema:   Si   H f   ( ~ x )   es una matriz denida positiva para todo   ~ x   ‚àà   Œ© , entonces la funci√≥n   f   es  estrictamente convexa   en   Œ© .  Teorema:   La funci√≥n   f   es convexa en   S   ‚äÜ   R n , conjunto convexo y no vac√≠o, si y s√≥lo si   f   ( ~ y )   ‚â•  f   ( ~ x ) +   ‚àá f   ( x ) T   ( ~ y   ‚àí   ~ x ) ,   ‚àÄ ~ x, ~ y   ‚àà   S .  Denici√≥n:   d   6   = 0   se dice   direcci√≥n de descenso   de   f   en   x   si existe   r   tal que   f   ( x + Œªd )   < f   ( x ) ,   ‚àÄ Œª   : 0   ‚â§   Œª   ‚â§   r . Si   d   es tal que   d T   ‚àá f   ( x )   <   0 , entonces   d   es una direcci√≥n de descenso. M√©todo del Gradiente o Descenso m√°s pronunciado de Cauchy: M√©todo iterativo que comienza en un punto factible y se aproxima a un punto que satisfaga condiciones locales de optimalidad, siguiendo  2  Una matriz cuadrada, invertible y sim√©trica   H f   es   semidenida positiva   si todos los determinantes de sus submatrices son positivos o cero. De manera similar, lo mismo ocurre si todos los valores propios de   H   son positivos o cero. Una matriz cuadrada, invertible y sim√©trica   H f   es   denida positiva   si todos los determinantes de sus sub- matrices son estrictamente positivos. De manera similar, lo mismo ocurre si todos los valores propios de   H   son estrictamente positivos. Una matriz cuadrada, invertible y sim√©trica   H f   es   semidenida negativa   si los determinantes de sus subma- trices   A n √ó n   con   n   impar son negativos o cero, y todos los determinantes de sus submatrices   A n √ó n   con   n   par son positivos o cero. De manera similar, lo mismo ocurre si todos los valores propios de   H   son negativos o cero. Una matriz cuadrada, invertible y sim√©trica   H f   es   denida negativa   si los determinantes de sus submatrices  A n √ó n   con   n   impar son estricamente negativos, y todos los determinantes de sus submatrices   A n √ó n   con   n   par son estrictamente positivos. De manera similar, lo mismo ocurre si todos los valores propios de   H   son estrictamente negativos.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  21

--- Page 22 ---
Resumen Optimizaci√≥n - ICS1113  una direcci√≥n de movimiento por iteraci√≥n. I) Comenzar el algoritmo con un punto factible inicial   ~ x k   =   ~ x 0   ‚àà   R n . II) Determinar la   direcci√≥n de descenso   o direcci√≥n de m√°ximo decrecimiento. Esta queda deter- minada por   ~ d k   =   ‚àí‚àá f   ( ~ x k ) . III) Si   ~ d k   =   ~ 0   terminar de iterar, pues tenemos un punto estacionario de   f   . Si no, resolver el problema:  m¬¥ ƒ±n   f   ( ~ x k   +   t k   ¬∑   ~ d k ) =   f   ( ~ x k   ‚àí   t k   ¬∑ ‚àá f   ( ~ x k ) )  s.a.   t >   0  t k   se denomina   tama√±o del paso . IV) Actualizar el punto   ~ x k +1   ‚Üê   ~ x k   +   t k   ¬∑   ~ h k , y la iteraci√≥n   k   ‚Üê   k   + 1 . Volver al paso II. Es posible determinar otros criterios de parada, como que   ||‚àá f   ( ~ x k ) || ‚â§      ,   || ~ x k +1   ‚àí   ~ x k || ‚â§    ,  || f   ( ~ x k +1 )   ‚àí   f   ( ~ x k ) || ‚â§    , o   k > r . Este algoritmo es de convergencia lineal. Este algoritmo cumple la propiedad de que   ( ~ d k ) T   ( ~ d k +1 ) = 0 , es decir, el m√©todo avanza en direc- ciones mutuamente ortogonales.  Teorema:   Sea   ~ x ‚àó   m√≠nimo local de   f   . Entonces, existe   r >   0   tal que si el m√©todo del gradiente es iniciado desde un punto   ~ x 0   tal que   || ~ x ‚àó   ‚àí   ~ x 0 ||   < r , este converge a   ~ x ‚àó . Luego, si   f   es estrictamente convexa y tiene m√≠nimo, entonces el m√©todo converge desde cualquier punto de partida. M√©todo de Newton: M√©todo iterativo que comienza en un punto factible y se aproxima a un punto que satisfaga condiciones locales de optimalidad, siguiendo una direcci√≥n de movimiento por iteraci√≥n. Utiliza informaci√≥n de segundo orden.  Algoritmo de Newton: (primera versi√≥n)  I) Comenzar el algoritmo con un punto factible inicial   ~ x k   =   ~ x 0   ‚àà   R n . II) Determinar el   ‚àá f   ( ~ x k ) =   ~ 0 . Si resulta ser igual a   ~ 0 , entonces terminar de iterar, pues tenemos un punto estacionario de   f   . Si no, continuar. III) Actualizar el punto   ~ x k +1   ‚Üê   ~ x k   ‚àí   [ ‚àá 2 f   ( ~ x k )] ‚àí 1   ¬∑ ‚àá f   ( ~ x k ) , y la iteraci√≥n   k   ‚Üê   k   + 1 . Volver al paso II).  Algoritmo de Newton: (segunda versi√≥n)  I) Comenzar el algoritmo con un punto factible inicial   ~ x k   =   ~ x 0   ‚àà   R n . II) Determinar el   ‚àá f   ( ~ x k ) =   ~ 0 . Si resulta ser igual a   ~ 0 , entonces terminar de iterar, pues tenemos un punto estacionario de   f   . Si no, continuar.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  22

--- Page 23 ---
Resumen Optimizaci√≥n - ICS1113  III) Sea   ~ d k   =   ‚àí [ ‚àá 2 f   ( ~ x k )] ‚àí 1   ¬∑ ‚àá f   ( ~ x k )   la direcci√≥n de descenso a utilizar. Resolver el problema:  m¬¥ ƒ±n   f   ( ~ x k   +   t k   ¬∑   ~ d k ) =   f   ( ~ x k   ‚àí   t k   ¬∑   [ ‚àá 2 f   ( ~ x k )] ‚àí 1   ¬∑ ‚àá f   ( ~ x k ) )  s.a.   t   ‚â•   0  t k   se denomina   tama√±o del paso . IV) Actualizar el punto   ~ x k +1   ‚Üê   ~ x k   +   t k   ¬∑   h k , y la iteraci√≥n   k   ‚Üê   k   + 1 . Volver al paso II). Se pueden determinar otro criterios de parada. El algoritmo converge cuadr√°ticamente. Si la funci√≥n es cuadr√°tica, el m√©todo converge en una iteraci√≥n.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  23

--- Page 24 ---
Resumen Optimizaci√≥n - ICS1113  Cap√≠tulo 11.- Optimizaci√≥n No Lineal Restringida.  Esta secci√≥n considerar√° un problema est√°ndar  m¬¥ ƒ±n   f   ( ~ x )  s.a.   g i ( ~ x )   ‚â§   0   ‚àÄ i   = 1 , . . . , m h j   ( ~ x ) = 0   ‚àÄ j   = 1 , . . . , r ~ x   ‚àà   R n  Denotaremos   el conjunto de soluciones factibles de   P   )   como   Œ© =   { ~ x   ‚àà   R n   :   g i ( ~ x )   ‚â§   b i ,   ‚àÄ i   = 1 , . . . , m } .  Denici√≥n:   Un   conjunto de √≠ndices activos   I ( ~ x )   (o, simplemente, conjunto activo) es el conjunto de los √≠ndices de las restricciones que est√°n activas en un punto:   I ( ~ x ) =   { i   ‚àà { 1 , . . . , m }   :   g i ( ~ x ) =   b i  } .  Denici√≥n:   Dado los vectores   ~ v 1 , . . . , ~ v r , el   cono generado por   ~ v 1 , . . . , ~ v r   es el conjunto  { x   :   x   =  r ‚àë  i =1  Œª i   ~ v i , Œª i   ‚â•   0 ,   ‚àÄ i   = 1 , . . . , r }  Denici√≥n:   Sea   ~ x   ‚àà   Œ© . Un vector   ~ d   ‚àà   R n ,  ~ d   6   = 0   es una   direcci√≥n factible   para   Œ©   con respecto   ~ x  si existe    >   0   tal que   ( ~ x   +   Œª ~ d )   ‚àà   Œ© ,   ‚àÄ Œª   ‚àà   (0 ,  ] .  Denici√≥n:   El   conjunto de direcciones factibles en   ~ x   es el conjunto   D ( ~ x ) =   { ~ d   ‚àà   R n   :   ~ d   es direcci√≥n factible de   Œ©   con respecto a   ~ x } .  Denici√≥n:   Un   cono tangente   T   ( ~ x )   es el cono generado por los gradientes de las restricciones activas en ese punto, denido como   T   ( ~ x ) =   { ~ h   ‚àà   R n   :   ‚àá g i ( ~ x ) T   ¬∑ ~ h   ‚â§   0 ,   ‚àÄ i   ‚àà   I ( ~ x ) } . Las direcciones factibles est√°n en el cono tangente   T   ( ~ x ) . Caso Unidimensional:   P   )   m¬¥ ƒ±n   f   ( x )  s.a.   a   ‚â§   x   ‚â§   b x   ‚àà   R  Teorema:   (Condici√≥n Necesaria de Primer Orden)   Sea   f   :   R   ‚Üí   R . Si   x ‚àó   ‚àà   [ a, b ]   es un punto m√≠nimo local de   P   ) , entonces Si   x ‚àó   =   a   ‚áí   f   ‚Ä≤ ( x ‚àó )   ‚â•   0 . Si   x ‚àó   =   b   ‚áí   f   ‚Ä≤ ( x ‚àó )   ‚â§   0 . Si   a < x ‚àó   < b   ‚áí   f   ‚Ä≤ ( x ‚àó ) = 0 .  Teorema:   (Condici√≥n Suciente de Primer Orden)   Sea   f   :   R   ‚Üí   R . Si   x ‚àó   ‚àà   [ a, b ]   es un punto m√≠nimo local de   P   ) , entonces Si   x ‚àó   =   a   y   f   ‚Ä≤ ( x ‚àó )   >   0   ‚áí   x ‚àó   es m√≠nimo local estricto de   P   ) .  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  24

--- Page 25 ---
Resumen Optimizaci√≥n - ICS1113  Si   x ‚àó   =   b   y   f   ‚Ä≤ ( x ‚àó )   <   0   ‚áí   x ‚àó   es un m√≠nimo local estricto de   P   ) .  Teorema:   (Condici√≥n Necesaria de Segundo Orden)   Sea   f   :   R   ‚Üí   R , una funci√≥n dos veces diferenciable. Si   x ‚àó   ‚àà   [ a, b ]   es un punto m√≠nimo local de   P   ) , entonces Si   x ‚àó   =   a   ‚áí   ( f   ‚Ä≤ ( a )   ‚â•   0 )   ‚à®   ( f   ‚Ä≤‚Ä≤ ( a )   ‚â•   0   si   f   ‚Ä≤ ( a ) = 0 ) . Si   x ‚àó   =   b   ‚áí   ( f   ‚Ä≤ ( b )   ‚â§   0 )   ‚à®   ( f   ‚Ä≤‚Ä≤ ( b )   ‚â•   0   si   f   ‚Ä≤ ( b ) = 0 ) . Si   a < x ‚àó   < b   ‚áí   f   ‚Ä≤ ( x ‚àó ) = 0   ‚àß   f   ‚Ä≤‚Ä≤ ( x ‚àó )   ‚â•   0 .  Teorema:   (Condici√≥n Suciente de Segundo Orden)   Sea   f   :   R   ‚Üí   R   perteneciente a la clase   C 2 . Sea   x ‚àó   ‚àà   [ a, b ] . Si   x ‚àó   =   a   ‚àß  (  f   ‚Ä≤ ( a )   >   0   ‚à®   ( f   ‚Ä≤ ( a ) = 0   ‚àß   f   ‚Ä≤‚Ä≤ ( a )   >   0 ) )  ‚áí   x ‚àó   es m√≠nimo local estricto de   P   ) . Si   x ‚àó   =   b   ‚àß  (  f   ‚Ä≤ ( b )   <   0   ‚à®   ( f   ‚Ä≤ ( b ) = 0   ‚àß   f   ‚Ä≤‚Ä≤ ( b )   >   0 ) )  ‚áí   x ‚àó   es m√≠nimo local estricto de   P   ) . Si   a < x ‚àó   < b   ‚àß   ( f   ‚Ä≤ ( x ‚àó ) = 0   ‚àß   f   ‚Ä≤‚Ä≤ ( x ‚àó )   >   0 )   ‚áí   x ‚àó   es m√≠nimo local estricto de   P   ) . Si se tiene m√°s de un m√≠nimo local, se deben comparar todos para encontrar el m√≠nimo global del problema   P   ) . Problema con Restricciones de Igualdad:  P   )   m¬¥ ƒ±n   f   ( ~ x )  s.a.   h i ( ~ x ) =   a i   ‚àÄ i   = 1 , . . . , r x   ‚àà   R n  Teorema de Lagrange:   (Condici√≥n Necesaria de Primer Orden)   El   Teorema de Lagrange   dice que si   ~ x ‚àó , punto factible regular, es m√≠nimo local de   P   ) , entonces existen   multiplicadores de Lagrange  Œª 1 , . . . , Œª m   tales que  ‚àá f   ( ~ x ‚àó ) +  r ‚àë  j =1  Œª j   ‚àá h j   ( ~ x ‚àó ) = 0  Denici√≥n:   La funci√≥n   L   a continuaci√≥n se llama funci√≥n de Lagrange, funci√≥n Lagrangeana o  Lagrangeano :  L ( ~ x,  ~ Œª ) =   f   ( ~ x ) +  r ‚àë  j =1  Œª j  ( h j   ( ~ x )   ‚àí   a j  )  Es posible reescribir el problema como  m¬¥ ƒ±n  ( ~ x, ~ Œª )  L ( ~ x,  ~ Œª )  s.a.   ~ x   ‚àà   R n  ~ Œª   ‚àà   R r  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  25

--- Page 26 ---
Resumen Optimizaci√≥n - ICS1113  Luego,   ( ~ x ‚àó ,  ~ Œª ‚àó )   es un punto estacionario del Lagrangeano. As√≠,   ‚àá x L   =   ~ 0   y   ‚àá Œª L   =   h ( ~ x )   ‚àí   a   =   ~ 0 .  Denici√≥n:   Se dice que el punto   ~ x   es   regular   (o que cumple las condiciones de regularidad) si el Jacobiano 3   J   ( ~ x )   de las restricciones evaluado en el punto es de rango m√°ximo   r , es decir, las   r  columnas son linealmente independientes. De lo contrario, se dice que el punto es   singular   . Para que los puntos estacionarios   ~ x ‚àó   sean √≥ptimos locales, debe cumplirse que  ‚àÜ x ‚àÇ 2 L ( ~ x ‚àó ,  ~ Œª ‚àó )  ‚àÇx 2   ‚àÜ x T   ‚â•   0  As√≠, la   condici√≥n necesaria de segundo orden   ser√° que el Hessiano del Lagrangeano sea semidenido positivo en el subespacio denido por las restricciones. Y la   condici√≥n suciente de segundo orden  ser√° que el Hessiano del Lagrangeano debe ser denido positivo en dicho subespacio. En este caso, el punto ser√° m√≠nimo local estricto. Luego, basta evaluar los puntos en el Hessiano para determinar la naturaleza de los puntos. Interpretaci√≥n de los Multiplicadores de Lagrange: El valor   Œª i   es la sensibilidad del valor √≥ptimo o  precio sombra   frente a una variaci√≥n unitaria en el par√°metro   a i   de la restricci√≥n   i . Si   ~ x ‚àó   es un punto m√≠nimo local:   ‚àÇ L ( ~ x ‚àó )  ‚àÇa i  =   ‚àÇf   ( ~ x ‚àó )  ‚àÇa i  =   ‚àí Œª i  Problema con Restricciones de Desigualdad: El problema diere en que las condiciones de optimalidad son distintas si se trata de un punto en el borde del dominio (restricciones activas) o de un punto interior (todas inactivas).  P   )   m¬¥ ƒ±n   f   ( ~ x )  s.a.   g i ( ~ x )   ‚â§   b i   ‚àÄ i   = 1 , . . . , m ~ x   ‚àà   R n  Teorema:   Sea   ~ x   ‚àà   Œ© , un m√≠nimo local del problema   P   ) . Entonces,   ‚àá T   f   ( ~ x ) d   ‚â•   0 ,   ‚àÄ d   ‚àà   D ( ~ x ) .  Teorema:   La   condici√≥n necesaria de Karush-Kuhn-Tucker (KKT)   dice que para un   ~ x ‚àó   m√≠nimo local del problema   P   ) , si se cumple la condici√≥n de regularidad en el punto   ~ x ‚àó , entonces existen multiplicadores   Œº 1   ‚â•   0 , Œº 2   ‚â•   0 , . . . , Œº m   ‚â•   0   tales que:  ‚àá f   ( ~ x ‚àó ) +  m ‚àë  i =1  Œº i ‚àá g i ( ~ x ‚àó ) = 0  Œº i  ( g i (   ~ x ‚àó )   ‚àí   b i  )   = 0 ,   ‚àÄ i   = 1 , . . . , m   Condiciones de holgura complementaria Estas son las denominadas   condiciones de KKT   .  3 J   ( ~ x ) =  Ô£Æ Ô£Ø Ô£Ø Ô£Ø Ô£∞  ‚àá h 1 ( ~ x )  ‚àá h 2 ( ~ x )  . . .  ‚àá h m ( ~ x )  Ô£π Ô£∫ Ô£∫ Ô£∫ Ô£ª  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  26

--- Page 27 ---
Resumen Optimizaci√≥n - ICS1113  Denici√≥n:   Se dice que el punto   ~ x   cumple con la   condici√≥n de regularidad   (o es regular) si el conjunto   T   ( ~ x )   dene al conjunto de las direcciones factibles.  Denici√≥n:   Se dice que el punto   ~ x   es   regular para las restricciones del problema   si los gradientes  ‚àá g i ( ~ x ) , i   ‚àà   I ( ~ x )   son linealmente independientes. Es decir, si el Jacobiano de las restricciones activas tiene rango completo. El problema se puede redenir con el uso del   Lagrangeano   del problema:  L ( ~ x, ~ Œº ) =   f   ( ~ x ) +  m ‚àë  i =1  Œº i  ( g i ( ~ x )   ‚àí   b i  )  en el que   ( ~ x ‚àó , ~ Œº ‚àó )   es punto estacionario del problema   m¬¥ ƒ±n ( ~ x,~ Œº ) ,~ Œº ‚â• ~ 0   L ( ~ x, ~ Œº ) . Luego, las condiciones de KKT necesarias para el siguiente problema de minimizacion/maximizaci√≥n 4  P   )   m¬¥ ƒ±n m¬¥ ax   f   ( ~ x )  s.a.   g i ( ~ x )   ‚â§   b i   ‚àÄ i   = 1 , . . . , m x j   ‚â•   0   j   ‚àà   J, J   ‚äÜ { 1 , . . . , n }  y usando el Lagrangeano   L ( ~ x, ~ Œº ) =   f   ( ~ x ) +  m ‚àë  i =1  Œº i  ( g i ( ~ x )   ‚àí   b i  ) , son  1 ,..., card ( J ) )   ‚àÇ L  ‚àÇx j  ‚â•   0  ‚â§   0   ‚àÄ j   ‚àà   J  card ( J )+1 ,...,n )   ‚àÇ L  ‚àÇx j  = 0   ‚àÄ j / ‚àà   J  n +1 ...n + m )   ‚àÇ L  ‚àÇŒº i  ‚â§   0   ‚àÄ i   = 1 , . . . , m  n + m +1 ...n + m + card ( J ) )   x j  ‚àÇ L  ‚àÇx j  = 0   ‚àÄ j   ‚àà   J  2 n + m +1 ... 2 n +2 m )   Œº i  ‚àÇ L  ‚àÇŒº i  = 0   ‚àÄ i   = 1 , . . . , m  2 n +2 m + card ( J ) )   x j   ‚â•   0   ‚àÄ j   ‚àà   J  2 n +3 m + card ( J ) )   Œº i  ‚â•   0  ‚â§   0   ‚àÄ i   = 1 , . . . , m  Teorema:   (Suciencia de las condiciones de KKT)   Si   ~ x ‚àó , soluci√≥n factible de   P   ) , es un punto que satisface las condiciones de KKT, y   P   )   es un problema convexo, encontes ese punto es m√≠nimo global del problema (no requiere regularidad del punto √≥ptimo).  Teorema:   (Condici√≥n de segundo orden de KKT)   Sea   ~ x ‚àó   punto regular tal que se cumple KKT con multiplicadores   ~ Œª ‚àó . Se tiene que:  4 Puede ser m√°s recomendable convertir el problema de   m¬¥ ax   f   ( ~ x )   a   ‚àí   m¬¥ ƒ±n   ‚àí f   ( ~ x ) , y continuar con las condi- ciones de m√≠nimo.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  27

--- Page 28 ---
Resumen Optimizaci√≥n - ICS1113  Si   ~ x ‚àó   es m√≠nimo local, entonces  ~ d T   ‚àÇ 2 L ( ~ x ‚àó ,  ~ Œª ‚àó )  ‚àÇx 2   ~ d   ‚â•   0 ,   ‚àÄ ~ d   ‚àà   K ( ~ x ‚àó )  Por otra parte, si  ~ d T   ‚àÇ 2 L ( ~ x ‚àó ,  ~ Œª ‚àó )  ‚àÇx 2   ~ d >   0 ,   ‚àÄ ~ d   ‚àà   K ( ~ x ‚àó ) ,  ~ d   6   =   ~ 0  entonces   ~ x ‚àó   es m√≠nimo local estricto del problema.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  28

--- Page 29 ---
Resumen Optimizaci√≥n - ICS1113  Cap√≠tulo 12.- Programaci√≥n Din√°mica.  Programaci√≥n Din√°mica es un cap√≠tulo que suele estar en el programa del curso ICS1113. Sin embar- go, el semestre 2016'2 se excluy√≥ del programa. Sin embargo, esta secci√≥n del resumen permanece para futuras referencias. Programaci√≥n din√°mica se basa en el Principio de Optimalidad de Bellman, y consiste en solucionar el presente suponiendo que en cada etapa futura del problema siempre se tomar√°n las decisiones correctas.  Teorema:   Principio de Optimalidad de Bellman:   Sea   x 1 , x 2 , . . . , x n   las decisiones √≥ptimas de cada per√≠odo, para un problema de   n   per√≠odos. Entonces,   x k , x k +1 , . . . , x n   son decisiones √≥ptimas para un problema de   n   ‚àí   k   + 1   per√≠odos. Para que un problema pueda ser resuelto con esta t√©cnica, debe cumplir con ciertas   caracter√≠sticas : Naturaleza secuencial de las decisiones: El problema puede ser dividido en etapas. Cada etapa tiene un n√∫mero de estados asociados a ella. La decisi√≥n √≥ptima de cada etapa depende s√≥lo del estado actual y no de las decisiones anteriores. La decisi√≥n tomada en una etapa determina cu√°l ser√° el estado de la etapa siguiente. As√≠, la pol√≠tica √≥ptima desde un estado   s   de la etapa   k   a la etapa nal est√° constituida por una decisi√≥n que transforma   s   en un estado   s ‚Ä≤   de la etapa   k   + 1   y por la pol√≠tica √≥ptima desde el estado  s ‚Ä≤   hasta la etapa nal. Para resolver el problema, se siguen los siguientes pasos: I)   Identicaci√≥n de etapas, estados y variables de decisi√≥n : Cada etapa debe tener asociado una o mas decisiones (problema de optimizaci√≥n), cuya dependencia de las decisiones anteriores esta dada exclusivamente por las variables de estado. Cada estado debe contener toda la informaci√≥n relevante para la toma de decisi√≥n aso- ciada al per√≠odo. Las variables de decisi√≥n son aquellas sobre las cuales debemos denir su valor de modo de optimizar el benecio acumulado y modicar el estado de la pr√≥xima etapa. II)   Descripci√≥n de ecuaciones de recurrencia : Deben indicar c√≥mo se acumula la funci√≥n objetivo y como var√≠an las funciones de estado de una etapa a otra. III)   Resoluci√≥n   Se debe optimizar cada subproblema por etapas en funci√≥n de los resultados de la resoluci√≥n del subproblema siguiente. Notar que las para que las recurrencias est√©n bien denidas, se requiere de condiciones borde.  Aclaraci√≥n 1:   Claramente, este RESUMEN no abarca toda la materia en exhaustividad.  Aclaraci√≥n 2:   Es posible que algo est√© mal tipeado.  Prof. Jos√© Tom√°s Marquinez V. -   B   jtmarquinezv@ing.puc.cl   - 2016  29

